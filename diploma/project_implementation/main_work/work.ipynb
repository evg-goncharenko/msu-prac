{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjEH-ZpRzcrk",
        "outputId": "4cf64c0a-9ac7-47e7-ff72-d1096f4dbe30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество вопросов:  6\n",
            "Количество ответов:  6\n"
          ]
        }
      ],
      "source": [
        "import ijson\n",
        "import os\n",
        "from setuptools import setup\n",
        "\n",
        "def parse_large_json(file_path):\n",
        "    # Открытие файла для потоковой обработки\n",
        "    answ_list = []\n",
        "\n",
        "    with open(file_path, 'rb') as file:\n",
        "        # Потоковая обработка элементов JSON\n",
        "        parser = ijson.parse(file)\n",
        "\n",
        "        for prefix, event, value in parser:\n",
        "            # тут нас интересуют элементы списков в json и событие по нахождению какого-либо ключа в этом списке\n",
        "            if prefix.endswith('.item') and event == 'map_key':\n",
        "                # заготовка под объект json, который будем строить\n",
        "                json_object = {}\n",
        "                # пока в текущем списке есть элементы, будем их анализировать\n",
        "\n",
        "                while event == 'map_key':\n",
        "                    # ijson идет итеративно по ключам и значениям\n",
        "                    # поэтому сначала сохраняем уже полученный ключ,\n",
        "                    # а потом запускаем следующую итерацию для получения значения\n",
        "                    key = value\n",
        "                    _, _, value = next(parser)\n",
        "                        \n",
        "\n",
        "                    answ_list.append(value)\n",
        "                    json_object[key] = value\n",
        "                    # для продолжения нашего while нам нужно снова перейти на следующий ключ списка\n",
        "                    # поэтому в этой итерации мы специально сеттим event (там как раз будет map_key)\n",
        "                    prefix, event, value = next(parser)\n",
        "\n",
        "                # с построенным объектом json можем делать что угодно, тут мы сверяем его с фильтром и записываем\n",
        "\n",
        "    return answ_list\n",
        "\n",
        "# full_dir - полный путь до json файлов\n",
        "full_dir = os.path.dirname(os.path.realpath('__file__'))\n",
        "\n",
        "answers = parse_large_json(full_dir + '/short/answers_short.json')\n",
        "questions = parse_large_json(full_dir + '/short/questions_short.json')\n",
        "\n",
        "print('Количество вопросов: ', len(questions))\n",
        "print('Количество ответов: ', len(answers))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4T8AwvmyaS2"
      },
      "source": [
        "\n",
        "\n",
        "### Разбиение данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество вопросов на обучение:  5\n",
            "Количество ответов на обучение:  5\n",
            "Количество вопросов на валидацию:  1\n",
            "Количество ответов на валидацию:  1\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Разбиение датасета на обучающий и валидационный наборы\n",
        "# В этом примере `test_size=0.1` указывает, что 10% данных будут использоваться для валидации, а оставшиеся 90% — для обучения. `random_state` обеспечивает воспроизводимость разбиения\n",
        "questions_train, questions_val, answers_train, answers_val = train_test_split(\n",
        "    questions, answers, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Создаем список меток, 0 или 1, в зависимости от того, насколько хорошо ответ соответствует вопросу\n",
        "labels_list = [1] * len(questions) # по умолчанию считаем, что все ответы соответствуют вопросу\n",
        "\n",
        "print('Количество вопросов на обучение: ', len(questions_train))\n",
        "print('Количество ответов на обучение: ', len(answers_train))\n",
        "\n",
        "print('Количество вопросов на валидацию: ', len(questions_val))\n",
        "print('Количество ответов на валидацию: ', len(answers_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Чтобы получить train_dataloader и val_dataloader для обучения и валидации модели BERT мы подготавливаем данные, токенизируем их и обварачиваем в DataLoader объекты из PyTorch. DataLoader позволяет итерировать по датасету с заданным размером батча.\n",
        "\n",
        "### Подготовка и токенизация данных\n",
        "\n",
        "Для начала токенизируем вопросы и ответы. Заготовленные данные questions_train, answers_train, questions_val, и answers_val у нас уже есть.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def encode_questions_answers(questions, answers, tokenizer, max_length=512):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for question, answer in zip(questions, answers):\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            text=question,  # Вопрос для кодирования\n",
        "            text_pair=answer,  # Ответ для кодирования\n",
        "            add_special_tokens=True,  # Добавление '[CLS]' и '[SEP]'\n",
        "            max_length=max_length,  # Ограничение длины входных данных\n",
        "            pad_to_max_length=True,  # Добавление паддинга до max_length\n",
        "            return_attention_mask=True,  # Возврат маски внимания\n",
        "            return_tensors='pt',  # Возврат pytorch тензоров\n",
        "        )\n",
        "        \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "# Кодирование данных обучения и валидации\n",
        "input_ids_train, attention_masks_train = encode_questions_answers(questions_train, answers_train, tokenizer)\n",
        "input_ids_val, attention_masks_val = encode_questions_answers(questions_val, answers_val, tokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Важные моменты:\n",
        "\n",
        "- Указываем max_length в encode_plus, соответствующую ограничениям нашей модели (например, 512 для BERT).\n",
        "- Если нет меток (labels_train и labels_val), то необходимо их создать в соответствии с задачей (например, классификация или вопросно-ответная система).\n",
        "- Размер батча (batch_size) можно настроить в зависимости от объема доступной памяти на устройстве. Больший размер батча ускоряет обучение, но требует больше памяти.\n",
        "\n",
        "С этими DataLoader объектами можно выполнять обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rcEE3_AyBk0",
        "outputId": "df14da3f-95d9-4f20-d6df-4c9592e7f278"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "num_epochs = 2\n",
        "batch_size = 10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, questions, answers, tokenizer, labels=None, max_length=512):\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "        self.labels = labels  # Метки классов для пар вопрос-ответ\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Кодирование вопроса и ответа\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            self.questions[idx],\n",
        "            self.answers[idx],\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Для данной задачи не нужны start_positions и end_positions,\n",
        "        # так как мы не пытаемся найти ответ в контексте\n",
        "        item = {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "        \n",
        "        # Добавление метки, если она есть:\n",
        "        if self.labels:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        \n",
        "        return item\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Создание Dataset\n",
        "# Предположим labels уже определены\n",
        "train_dataset = QADataset(questions_train, answers_train, tokenizer, labels_list)\n",
        "val_dataset = QADataset(questions_val, answers_val, tokenizer, labels_list)\n",
        "\n",
        "# Создание DataLoader'ов\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,  # датасет для обучения\n",
        "    batch_size=batch_size, # размер батча\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,  # датасет для валидации\n",
        "    batch_size=batch_size,  # размер батча\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "# Инициализация модели\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
        "# num_labels=2 - допустим, что у нас два класса\n",
        "\n",
        "# Оптимизатор\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Получение значений потерь и точности\n",
        "\n",
        "Чтобы получить значения потерь и точности (или других метрик) на этапах обучения и валидации, нам нужно вычислить их по окончании каждой эпохи обучения.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], [] # инициализация списков для хранения метрик\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Цикл по эпохам обучения, где мы будем вычислять потери и точность на обучающем и валидационном наборах данных:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.8141286969184875\n",
            "Epoch 1, Loss: 0.30838146805763245\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Переключение модели в режим обучения\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    # Обучение на тренировочном наборе\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0UAXaCBwyP-l"
      },
      "outputs": [],
      "source": [
        "def prepare_encodings(tokenizer, question, answer, device, max_length=512):\n",
        "    # Объединяем вопрос и ответ в одну последовательность\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        question,\n",
        "        answer,\n",
        "        add_special_tokens=True, # Добавляет токены [CLS] и [SEP]\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "    return encoding.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_most_relevant_answer(model, tokenizer, question, answers, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    \n",
        "    max_relevance = float('-inf')\n",
        "    most_relevant_answer = \"\"\n",
        "    \n",
        "    for answer in answers:\n",
        "        inputs = tokenizer.encode_plus(question, answer, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        \n",
        "        # Допустим, что мера релевантности вычисляется как среднее между максимальными значениями start_logits и end_logits\n",
        "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "        relevance = (torch.max(start_logits) + torch.max(end_logits)) / 2\n",
        "        \n",
        "        if relevance > max_relevance:\n",
        "            max_relevance = relevance\n",
        "            most_relevant_answer = answer\n",
        "            \n",
        "    return most_relevant_answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Наши записанные ответы в answers:\n",
        "print(\"Записанные ответы:\", answers)\n",
        "\n",
        "# Наши записанные вопросы в answers:\n",
        "print(\"Записанные вопросы:\", questions)\n",
        "\n",
        "validation_questions = [\"При каких обстоятельствах возникает исключение IndexError?\", \n",
        "                        \"Когда возникает исключение NotImplementedError?\",\n",
        "                        \"Как проверить, что один кортеж содержит все элементы другого кортежа?\",\n",
        "                        \"Чувствителен ли Python к регистру?\",\n",
        "                        \"Что такое модули Python?\",\n",
        "                        \"Объясните, что означает «self» в Python.\"]\n",
        "\n",
        "\n",
        "for question in validation_questions:\n",
        "    most_relevant_answer = find_most_relevant_answer(model, tokenizer, question, answers, device)\n",
        "    print(\"Задаваемый вопрос: \", question)\n",
        "    print(\" Ответ: \", most_relevant_answer, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIju1W9jye5s"
      },
      "source": [
        "\n",
        "\n",
        "### Построение графиков потерь и точности\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEaNhwjkyfkZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# train_losses = []  # Список для хранения значений потерь на обучающем наборе\n",
        "# val_losses = []  # Список для хранения значений потерь на валидационном наборе\n",
        "# train_accuracies = []  # Список для хранения значений точности на обучающем наборе\n",
        "# val_accuracies = []  # Список для хранения значений точности на валидационном наборе\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Здесь должен быть ваш код для обучения модели и валидации\n",
        "    # Вместо реальных значений используются заглушки\n",
        "    train_loss, val_loss, train_accuracy, val_accuracy = epoch*0.1, epoch*0.11, epoch*0.1, epoch*0.12\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # График потерь\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Epoch: {}/{}'.format(epoch + 1, num_epochs))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # График точности\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
